{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27b78e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys \n",
    "sys.path.append(\"../\")\n",
    "import torch \n",
    "import datetime\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from argparse import Namespace\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import argparse\n",
    "import scipy \n",
    "from matplotlib.pyplot import cm\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import comet_ml\n",
    "import pandas as pd\n",
    "from glob2 import glob\n",
    "import copy\n",
    "import IPython\n",
    "\n",
    "import baseline.dataset_loaders.chime as chime\n",
    "import baseline.dataset_loaders.libri1to3chime as libri1to3chime\n",
    "import baseline.utils.mixture_consistency as mixture_consistency\n",
    "import baseline.models.improved_sudormrf as improved_sudormrf\n",
    "import baseline.metrics.dnnmos_metric as dnnmos_metric\n",
    "import baseline.metrics.sisdr_metric as sisdr_metric\n",
    "\n",
    "import pickle\n",
    "\n",
    "# from __config__ import *\n",
    "# plt.style.use('science')\n",
    "# plt.style.use(['science','ieee','no-latex'])\n",
    "# plt.style.reload_library()\n",
    "# plt.style.use(['science', 'ieee'])\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([\"0\", \"1\"])\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([\"2\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd0c5b-b7b2-4232-aceb-a365636a09d0",
   "metadata": {},
   "source": [
    "# Dev and Eval sets of CHiME5 (single-speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd82921a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 1\n",
    "sample_rate = 16000\n",
    "timelength = 4.\n",
    "fixed_n_sources = 1\n",
    "get_only_active_speakers = False\n",
    "use_vad = False\n",
    "random_order = True\n",
    "n_samples = 250\n",
    "time_samples = int(sample_rate * timelength)\n",
    "model_type = 'teacher'\n",
    "\n",
    "data_loader = chime.Dataset(\n",
    "    sample_rate=sample_rate, fixed_n_sources=fixed_n_sources,\n",
    "    timelength=timelength, augment=random_order,\n",
    "    zero_pad=True, split='dev', get_only_active_speakers=get_only_active_speakers,\n",
    "    normalize_audio=False, n_samples=n_samples, use_vad=use_vad)\n",
    "\n",
    "val_chime_gen = data_loader.get_generator(\n",
    "    batch_size=batch_size, num_workers=1) \n",
    "\n",
    "data_loader = chime.Dataset(\n",
    "    sample_rate=sample_rate, fixed_n_sources=fixed_n_sources,\n",
    "    timelength=timelength, augment=random_order,\n",
    "    zero_pad=True, split='eval', get_only_active_speakers=get_only_active_speakers,\n",
    "    normalize_audio=False, n_samples=n_samples, use_vad=use_vad)\n",
    "\n",
    "eval_chime_gen = data_loader.get_generator(\n",
    "    batch_size=batch_size, num_workers=1) \n",
    "\n",
    "def get_new_teacher(hparams, depth_growth):\n",
    "    student = improved_sudormrf.SuDORMRF(\n",
    "        out_channels=hparams[\"out_channels\"],\n",
    "        in_channels=hparams[\"in_channels\"],\n",
    "        num_blocks=int(depth_growth * hparams[\"num_blocks\"]),\n",
    "        upsampling_depth=hparams[\"upsampling_depth\"],\n",
    "        enc_kernel_size=hparams[\"enc_kernel_size\"],\n",
    "        enc_num_basis=hparams[\"enc_num_basis\"],\n",
    "        num_sources=2,\n",
    "    )\n",
    "    return student\n",
    "\n",
    "hparams = {\n",
    "    'out_channels': 256,\n",
    "    'in_channels': 512,\n",
    "    'num_blocks': 8,\n",
    "    'upsampling_depth': 7,\n",
    "    'enc_kernel_size': 81,\n",
    "    'enc_num_basis': 512,\n",
    "}\n",
    "\n",
    "def get_new_student(hparams, depth_growth):\n",
    "    student = improved_sudormrf.SuDORMRF(\n",
    "        out_channels=hparams[\"out_channels\"],\n",
    "        in_channels=hparams[\"in_channels\"],\n",
    "        num_blocks=int(depth_growth * hparams[\"num_blocks\"]),\n",
    "        upsampling_depth=hparams[\"upsampling_depth\"],\n",
    "        enc_kernel_size=hparams[\"enc_kernel_size\"],\n",
    "        enc_num_basis=hparams[\"enc_num_basis\"],\n",
    "        num_sources=2,\n",
    "    )\n",
    "    return student\n",
    "\n",
    "hparams = {\n",
    "    'out_channels': 256,\n",
    "    'in_channels': 512,\n",
    "    'num_blocks': 8,\n",
    "    'upsampling_depth': 7,\n",
    "    'enc_kernel_size': 81,\n",
    "    'enc_num_basis': 512,\n",
    "}\n",
    "\n",
    "if model_type == 'student':\n",
    "    w_mix_con_chkpt = '../pretrained_checkpoints/chime_adapted_remixit_student_w_mixconsist.pt'\n",
    "else:\n",
    "    w_mix_con_chkpt = \"../pretrained_checkpoints/libri1to3mix_supervised_teacher_w_mixconsist.pt\"\n",
    "\n",
    "w_mix_con_model = get_new_teacher(hparams, depth_growth=1)\n",
    "w_mix_con_model.load_state_dict(torch.load(w_mix_con_chkpt))\n",
    "#wo_mix_con_model = wo_mix_con_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f8290-498a-4762-9413-6bddf874b874",
   "metadata": {},
   "source": [
    "## CHiME (single-speaker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19fc5bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A/srv/storage/talc3@talc-data.nancy/multispeech/calcul/users/msadeghi/pytorch/CHiME2023/unsup_speech_enh_adaptation/notebooks/../baseline/dataset_loaders/abstract_dataset.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_wav = torch.tensor(\n",
      "\n",
      "\n",
      "1it [00:01,  1.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "2it [00:02,  1.38s/it]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:03,  1.34s/it]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:04,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:06,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:07,  1.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:08,  1.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:09,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "9it [00:10,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "10it [00:11,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:13,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "12it [00:14,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:15,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "14it [00:16,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "15it [00:18,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "16it [00:19,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "17it [00:20,  1.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "18it [00:21,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "19it [00:22,  1.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "20it [00:23,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "21it [00:24,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "22it [00:26,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "23it [00:27,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "24it [00:28,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "25it [00:29,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "26it [00:30,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "27it [00:31,  1.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "28it [00:32,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "29it [00:34,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "30it [00:35,  1.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "31it [00:35,  1.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "32it [00:36,  1.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "33it [00:38,  1.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "34it [00:39,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "35it [00:40,  1.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "36it [00:41,  1.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "37it [00:43,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "38it [00:44,  1.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "39it [00:45,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "40it [00:46,  1.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "41it [00:47,  1.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "42it [00:48,  1.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "43it [00:49,  1.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "44it [00:51,  1.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "45it [00:52,  1.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "46it [00:53,  1.15s/it]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m mix, est_s, est_n \u001b[38;5;241m=\u001b[39m input_mix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), teacher_est_active_speakers[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy(), teacher_est_noises[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Msasure the DNSMOS\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m dnsmos_val \u001b[38;5;241m=\u001b[39m \u001b[43mdnnmos_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dnsmos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mest_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dnsmos_val\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     30\u001b[0m     eval_res_dic[k]\u001b[38;5;241m.\u001b[39mappend(v)\n",
      "File \u001b[0;32m/srv/storage/talc3@talc-data.nancy/multispeech/calcul/users/msadeghi/pytorch/CHiME2023/unsup_speech_enh_adaptation/notebooks/../baseline/metrics/dnnmos_metric.py:67\u001b[0m, in \u001b[0;36mcompute_dnsmos\u001b[0;34m(audio, fs, input_length, _sessions)\u001b[0m\n\u001b[1;32m     64\u001b[0m input_features \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(audio_logpowspec(audio\u001b[38;5;241m=\u001b[39maudio_seg))\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)[np\u001b[38;5;241m.\u001b[39mnewaxis,:,:]\n\u001b[1;32m     66\u001b[0m onnx_inputs_sig \u001b[38;5;241m=\u001b[39m {inp\u001b[38;5;241m.\u001b[39mname: input_features \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m _sessions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_inputs()}\n\u001b[0;32m---> 67\u001b[0m mos_sig \u001b[38;5;241m=\u001b[39m poly\u001b[38;5;241m.\u001b[39mpolyval(\u001b[43m_sessions\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_inputs_sig\u001b[49m\u001b[43m)\u001b[49m, COEFS_SIG)\n\u001b[1;32m     69\u001b[0m onnx_inputs_bak_ovr \u001b[38;5;241m=\u001b[39m {inp\u001b[38;5;241m.\u001b[39mname: input_features \u001b[38;5;28;01mfor\u001b[39;00m inp \u001b[38;5;129;01min\u001b[39;00m _sessions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbak_ovr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_inputs()}\n\u001b[1;32m     70\u001b[0m mos_bak_ovr \u001b[38;5;241m=\u001b[39m _sessions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbak_ovr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28;01mNone\u001b[39;00m, onnx_inputs_bak_ovr)\n",
      "File \u001b[0;32m/srv/storage/talc3@talc-data.nancy/multispeech/calcul/users/msadeghi/anaconda3/envs/sudo/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:200\u001b[0m, in \u001b[0;36mSession.run\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m     output_names \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs_meta]\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_feed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m C\u001b[38;5;241m.\u001b[39mEPFail \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extract proper numbers for val and test for DNS-MOS\n",
    "eval_res_dic = {'sig_mos': [], 'bak_mos': [], 'ovr_mos': []}\n",
    "w_mix_con_model.eval()\n",
    "\n",
    "# Eval set\n",
    "for cnt, mixture in tqdm(enumerate(eval_chime_gen)):\n",
    "    input_mix = mixture.unsqueeze(1) #.cuda()\n",
    "    input_mix_std = input_mix.std(-1, keepdim=True)\n",
    "    input_mix_mean = input_mix.mean(-1, keepdim=True)\n",
    "    input_mix = (input_mix - input_mix_mean) / (input_mix_std + 1e-9)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_sources_wavs = w_mix_con_model(input_mix) \n",
    "        rec_sources_wavs = mixture_consistency.apply(rec_sources_wavs, input_mix)\n",
    "        \n",
    "        new_mix = rec_sources_wavs[:, 0:1] + rec_sources_wavs[:, 1:]\n",
    "        new_mix_std = new_mix.std(-1, keepdim=True)\n",
    "        new_mix_mean = new_mix.mean(-1, keepdim=True)\n",
    "        rec_sources_wavs = (rec_sources_wavs - new_mix_mean) / (new_mix_std + 1e-9)\n",
    "        \n",
    "#         rec_sources_wavs = (rec_sources_wavs * new_mix_std) + new_mix_mean\n",
    "        teacher_est_active_speakers = rec_sources_wavs[:, 0:1].detach().cpu()\n",
    "        teacher_est_noises = rec_sources_wavs[:, 1:].detach().cpu()\n",
    "\n",
    "    mix, est_s, est_n = input_mix[0, 0].cpu().numpy(), teacher_est_active_speakers[0, 0].cpu().numpy(), teacher_est_noises[0, 0].cpu().numpy()\n",
    "    \n",
    "    # Msasure the DNSMOS\n",
    "    dnsmos_val = dnnmos_metric.compute_dnsmos(est_s, fs=16000)\n",
    "    for k, v in dnsmos_val.items():\n",
    "        eval_res_dic[k].append(v)\n",
    "    \n",
    "    if cnt > n_samples:\n",
    "        break\n",
    "\n",
    "val_res_dic = {'sig_mos': [], 'bak_mos': [], 'ovr_mos': []}\n",
    "wo_mix_con_model.eval()\n",
    "\n",
    "# Dev set\n",
    "for cnt, mixture in tqdm(enumerate(val_chime_gen)):\n",
    "    input_mix = mixture.unsqueeze(1) #.cuda()\n",
    "    input_mix_std = input_mix.std(-1, keepdim=True)\n",
    "    input_mix_mean = input_mix.mean(-1, keepdim=True)\n",
    "    input_mix = (input_mix - input_mix_mean) / (input_mix_std + 1e-9)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_sources_wavs = w_mix_con_model(input_mix)\n",
    "        rec_sources_wavs = mixture_consistency.apply(rec_sources_wavs, input_mix)\n",
    "        \n",
    "        new_mix = rec_sources_wavs[:, 0:1] + rec_sources_wavs[:, 1:]\n",
    "        new_mix_std = new_mix.std(-1, keepdim=True)\n",
    "        new_mix_mean = new_mix.mean(-1, keepdim=True)\n",
    "        rec_sources_wavs = (rec_sources_wavs - new_mix_mean) / (new_mix_std + 1e-9)\n",
    "        \n",
    "#         rec_sources_wavs = (rec_sources_wavs * new_mix_std) + new_mix_mean\n",
    "        teacher_est_active_speakers = rec_sources_wavs[:, 0:1].detach().cpu()\n",
    "        teacher_est_noises = rec_sources_wavs[:, 1:].detach().cpu()\n",
    "\n",
    "    mix, est_s, est_n = input_mix[0, 0].cpu().numpy(), teacher_est_active_speakers[0, 0].cpu().numpy(), teacher_est_noises[0, 0].cpu().numpy()\n",
    "    \n",
    "    # Msasure the DNSMOS\n",
    "    dnsmos_val = dnnmos_metric.compute_dnsmos(est_s, fs=16000)\n",
    "    for k, v in dnsmos_val.items():\n",
    "        val_res_dic[k].append(v)\n",
    "    \n",
    "    if cnt > n_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75f3bac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val\n",
      "sig_mos 3.1403073230397824\n",
      "bak_mos 0.24783729047514014\n",
      "ovr_mos 2.0211982832454853\n",
      "eval\n",
      "sig_mos 3.0753744058996473\n",
      "bak_mos 0.2666276682215719\n",
      "ovr_mos 2.0101867086121867\n"
     ]
    }
   ],
   "source": [
    "for name, this_dic in [('val', val_res_dic), ('eval', eval_res_dic)]:\n",
    "    print(name)\n",
    "    for k, v in this_dic.items():\n",
    "        print(k, np.median(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb90887-8465-4521-9556-c7c7497eaf56",
   "metadata": {},
   "source": [
    "# Dev and Eval sets of LibriCHiME-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84a42927-0fa9-4614-8f97-93a35fac2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "sample_rate = 16000\n",
    "timelength = 4.0\n",
    "fixed_n_sources = -1\n",
    "split = 'dev'\n",
    "min_or_max = 'min'\n",
    "n_speakers_priors = [0.50, 0.25, 0.25]\n",
    "time_samples = int(sample_rate * timelength)\n",
    "random_order = True\n",
    "\n",
    "hparams = {\n",
    "    \"rescale_to_input_mixture\": False,\n",
    "    \"apply_mixture_consistency\": True,\n",
    "}\n",
    "\n",
    "data_loader = libri1to3chime.Dataset(\n",
    "    sample_rate=sample_rate, fixed_n_sources=fixed_n_sources,\n",
    "    timelength=timelength, augment=random_order,\n",
    "    zero_pad=True, min_or_max=min_or_max, split='dev',\n",
    "    normalize_audio=False, n_samples=-1,\n",
    "    n_speakers_priors=n_speakers_priors)\n",
    "\n",
    "dev_libriChime_gen = data_loader.get_generator(\n",
    "    batch_size=batch_size, num_workers=1)\n",
    "\n",
    "data_loader = libri1to3chime.Dataset(\n",
    "    sample_rate=sample_rate, fixed_n_sources=fixed_n_sources,\n",
    "    timelength=timelength, augment=random_order,\n",
    "    zero_pad=True, min_or_max=min_or_max, split='test',\n",
    "    normalize_audio=False, n_samples=-1,\n",
    "    n_speakers_priors=n_speakers_priors)\n",
    "\n",
    "eval_libriChime_gen = data_loader.get_generator(\n",
    "    batch_size=batch_size, num_workers=1)\n",
    "\n",
    "def apply_output_transform(rec_sources_wavs, input_mix_std,\n",
    "                           input_mix_mean, input_mom, hparams):\n",
    "    if hparams[\"rescale_to_input_mixture\"]:\n",
    "        rec_sources_wavs = (rec_sources_wavs * input_mix_std) + input_mix_mean\n",
    "    if hparams[\"apply_mixture_consistency\"]:\n",
    "        rec_sources_wavs = mixture_consistency.apply(rec_sources_wavs, input_mom)\n",
    "    return rec_sources_wavs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264f0d55-a118-4639-b565-d6b33db9af3b",
   "metadata": {},
   "source": [
    "## LibriChime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79695e51-0ad2-4f78-b700-647fa5594932",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                 | 0/3000 [00:00<?, ?it/s]\u001b[A\u001b[A/srv/storage/talc3@talc-data.nancy/multispeech/calcul/users/msadeghi/pytorch/CHiME2023/unsup_speech_enh_adaptation/notebooks/../baseline/dataset_loaders/abstract_dataset.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_wav = torch.tensor(\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                                                                                                                         | 1/3000 [00:00<35:17,  1.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▏                                                                                                                                                                                                                                                                                                                        | 2/3000 [00:00<26:20,  1.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▎                                                                                                                                                                                                                                                                                                                        | 3/3000 [00:00<19:58,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▍                                                                                                                                                                                                                                                                                                                        | 4/3000 [00:01<15:30,  3.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▌                                                                                                                                                                                                                                                                                                                        | 5/3000 [00:01<12:30,  3.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                 | 0/3000 [00:00<?, ?it/s]\u001b[A\u001b[A/srv/storage/talc3@talc-data.nancy/multispeech/calcul/users/msadeghi/pytorch/CHiME2023/unsup_speech_enh_adaptation/notebooks/../baseline/dataset_loaders/abstract_dataset.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor_wav = torch.tensor(\n",
      "\n",
      "\n",
      "  0%|                                                                                                                                                                                                                                                                                                                         | 1/3000 [00:00<09:04,  5.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▏                                                                                                                                                                                                                                                                                                                        | 2/3000 [00:00<07:56,  6.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▎                                                                                                                                                                                                                                                                                                                        | 3/3000 [00:00<07:08,  6.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|▌                                                                                                                                                                                                                                                                                                                        | 5/3000 [00:00<06:53,  7.25it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "eval_libriChime_res_dic = {'sig_mos': [], 'bak_mos': [], 'ovr_mos': [], 'si_sdr': [], 'si_sdri': []}\n",
    "dev_libriChime_res_dic = {'sig_mos': [], 'bak_mos': [], 'ovr_mos': [], 'si_sdr': [], 'si_sdri': []}\n",
    "\n",
    "w_mix_con_model.eval()\n",
    "\n",
    "cnt = 0\n",
    "n_samples = 5\n",
    "\n",
    "# Eval set\n",
    "for speakers, noise in tqdm(eval_libriChime_gen):\n",
    "    gt_speaker_mix = speakers.sum(1, keepdims=True) #.cuda()\n",
    "    # noise = noise.cuda()\n",
    "\n",
    "    input_mix = noise + gt_speaker_mix\n",
    "    input_mix_std = input_mix.std(-1, keepdim=True)\n",
    "    input_mix_mean = input_mix.mean(-1, keepdim=True)\n",
    "    input_mix = (input_mix - input_mix_mean) / (input_mix_std + 1e-9)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_sources_wavs = w_mix_con_model(input_mix)\n",
    "        rec_sources_wavs = apply_output_transform(\n",
    "            rec_sources_wavs, input_mix_std, input_mix_mean, input_mix, hparams)\n",
    "        teacher_est_active_speakers = rec_sources_wavs[:, 0:1]\n",
    "        teacher_est_noises = rec_sources_wavs[:, 1:]\n",
    "    \n",
    "    sisdr = sisdr_metric.compute_sisdr(\n",
    "        teacher_est_active_speakers.cpu().numpy(), gt_speaker_mix.cpu().numpy())\n",
    "    \n",
    "    mix_sisdr = sisdr - sisdr_metric.compute_sisdr(\n",
    "        input_mix.cpu().numpy(), gt_speaker_mix.cpu().numpy())\n",
    "    \n",
    "    eval_libriChime_res_dic['si_sdr'].append(sisdr)\n",
    "    eval_libriChime_res_dic['si_sdri'].append(mix_sisdr)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    if cnt > n_samples:\n",
    "        break\n",
    "        \n",
    "cnt = 0\n",
    "\n",
    "# Dev set\n",
    "for speakers, noise in tqdm(dev_libriChime_gen):\n",
    "    gt_speaker_mix = speakers.sum(1, keepdims=True) #.cuda()\n",
    "    # noise = noise.cuda()\n",
    "\n",
    "    input_mix = noise + gt_speaker_mix\n",
    "    input_mix_std = input_mix.std(-1, keepdim=True)\n",
    "    input_mix_mean = input_mix.mean(-1, keepdim=True)\n",
    "    input_mix = (input_mix - input_mix_mean) / (input_mix_std + 1e-9)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        rec_sources_wavs = w_mix_con_model(input_mix)\n",
    "        rec_sources_wavs = apply_output_transform(\n",
    "            rec_sources_wavs, input_mix_std, input_mix_mean, input_mix, hparams)\n",
    "        teacher_est_active_speakers = rec_sources_wavs[:, 0:1]\n",
    "        teacher_est_noises = rec_sources_wavs[:, 1:]\n",
    "    \n",
    "    sisdr = sisdr_metric.compute_sisdr(\n",
    "        teacher_est_active_speakers.cpu().numpy(), gt_speaker_mix.cpu().numpy())\n",
    "    \n",
    "    mix_sisdr = sisdr - sisdr_metric.compute_sisdr(\n",
    "        input_mix.cpu().numpy(), gt_speaker_mix.cpu().numpy())\n",
    "    \n",
    "    dev_libriChime_res_dic['si_sdr'].append(sisdr)\n",
    "    dev_libriChime_res_dic['si_sdri'].append(mix_sisdr)\n",
    "    \n",
    "    cnt += 1\n",
    "    \n",
    "    if cnt > n_samples:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f3a164e-8624-4743-8621-21ddf35ad4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sig_mos': [],\n",
       " 'bak_mos': [],\n",
       " 'ovr_mos': [],\n",
       " 'si_sdr': [19.673168659210205,\n",
       "  6.154797673225403,\n",
       "  19.270875453948975,\n",
       "  10.007598400115967,\n",
       "  21.264519691467285,\n",
       "  5.16119122505188],\n",
       " 'si_sdri': [5.310730934143066,\n",
       "  6.618217974901199,\n",
       "  6.380683183670044,\n",
       "  13.534106314182281,\n",
       "  8.796828985214233,\n",
       "  9.86205279827118]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_libriChime_res_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1191b8b-2234-489a-a337-ca11f7ec7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sig_mos': [],\n",
       " 'bak_mos': [],\n",
       " 'ovr_mos': [],\n",
       " 'si_sdr': [11.347124576568604,\n",
       "  21.56254291534424,\n",
       "  3.950425386428833,\n",
       "  23.122360706329346,\n",
       "  7.49070405960083,\n",
       "  15.223684310913086],\n",
       " 'si_sdri': [5.2526432275772095,\n",
       "  7.2285425662994385,\n",
       "  4.450291655957699,\n",
       "  10.25989294052124,\n",
       "  10.8815136551857,\n",
       "  2.763533592224121]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_libriChime_res_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f142a7d-692a-4625-b486-7eb4b43c6d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
